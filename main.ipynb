{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lifelines.datasets import load_rossi\n",
    "from lifelines.utils import concordance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rossi=load_rossi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=rossi.drop(columns=['week','arrest']).copy()\n",
    "E=rossi['arrest'].copy().values\n",
    "Y=rossi['week'].copy().values\n",
    "X_train, X_test, E_train, E_test, Y_train, Y_test = train_test_split(X, E, Y, test_size=0.2)\n",
    "\n",
    "scaler=StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train).astype(np.float32)\n",
    "X_test=scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "scaler=StandardScaler().fit(Y_train.reshape(-1,1))\n",
    "Y_train=scaler.transform(Y_train.reshape(-1,1)).flatten()\n",
    "Y_test=scaler.transform(Y_test.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(345, 7)\n",
      "(345,)\n",
      "(345,)\n",
      "(87, 7)\n",
      "(87,)\n",
      "(87,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(E_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(E_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define survival module\n",
    "class DeepSurv(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.surv_layers = nn.Sequential(\n",
    "            nn.Linear(input_shape, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(200, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(30),\n",
    "            nn.Linear(30, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        logits = self.surv_layers(inputs)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(E):\n",
    "    global device\n",
    "    E = torch.Tensor(E).to(device)\n",
    "    def loss(output, target):\n",
    "        hazard_ratio = torch.exp(output)\n",
    "        log_risk = torch.log(torch.cumsum(hazard_ratio, 0))\n",
    "        uncensored_likelihood = torch.transpose(output, 0, 1) - log_risk\n",
    "        censored_likelihood = uncensored_likelihood * E\n",
    "        neg_likelihood_ = -torch.sum(censored_likelihood)\n",
    "\n",
    "        # TODO\n",
    "        # For some reason, adding num_observed_events does not work.\n",
    "        # Therefore, for now we will use it as a simple factor of 1.\n",
    "        # Is it really needed? Isn't it just a scaling factor?\n",
    "        # num_observed_events = tf.math.cumsum(E)\n",
    "        # num_observed_events = tf.cast(num_observed_events, dtype=tf.float32)\n",
    "        # num_observed_events = torch.cumsum(E, 0)\n",
    "        num_observed_events = torch.ones([1,1],dtype=torch.float32).to(device)\n",
    "        \n",
    "        neg_likelihood = neg_likelihood_ / num_observed_events        \n",
    "        \n",
    "        return neg_likelihood\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(Y_train)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[sort_idx]\n",
    "E_train = E_train[sort_idx]\n",
    "Y_train = Y_train[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSurvData(Dataset):\n",
    "    def __init__(self, features, events, times):\n",
    "        assert len(features)==len(events)==len(times)\n",
    "        self.features = features\n",
    "        self.events = events\n",
    "        self.times = times\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.events[idx], self.times[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train=CustomSurvData(features=X_train, events=E_train, times=Y_train)\n",
    "dataset_test=CustomSurvData(features=X_test, events=E_test, times=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train_surv=DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=False)\n",
    "dataloader_test_surv=DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSurv(\n",
      "  (surv_layers): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=200, out_features=30, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "    (7): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DeepSurv(input_shape=X_train.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "optimizer = torch.optim.NAdam(model.parameters(),weight_decay=16)\n",
    "loss_train = negative_log_likelihood(E=E_train)\n",
    "loss_test = negative_log_likelihood(E=E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    global device\n",
    "    model.train()\n",
    "    for X, event, time in dataloader:\n",
    "        X = X.to(device)\n",
    "        event = event.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, event)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # raw predictions are risks\n",
    "        # the computation of lifelines.utils.concordance_index is based on the predicted scores\n",
    "        # therefore, the risks need to be converted\n",
    "        pred_score = np.exp(-pred.detach().to('cpu'))\n",
    "        c_index=concordance_index(event_times=time, predicted_scores=pred_score, event_observed=event.to('cpu'))\n",
    "\n",
    "        loss = loss.item()\n",
    "        print(f\"Training: \\tloss {loss:>7f},\\t C index {(c_index):>0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model: nn.Module, loss_fn):\n",
    "    global device\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, event, time in dataloader:\n",
    "            X = X.to(device)\n",
    "            event = event.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, event).item()\n",
    "\n",
    "            # raw predictions are risks\n",
    "            # the computation of lifelines.utils.concordance_index is based on the predicted scores\n",
    "            # therefore, the risks need to be converted\n",
    "            pred_score = np.exp(-pred.to('cpu'))\n",
    "            c_index=concordance_index(event_times=time, predicted_scores=pred_score, event_observed=event.to('cpu'))\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Testing: \\tAvg loss {test_loss:>8f},\\t C index {(c_index):>0.4f}\\n\")\n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training: \tloss 149764.468750,\t C index 0.4616\n",
      "Testing: \tAvg loss 8511.131836,\t C index 0.5929\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training: \tloss 141713.515625,\t C index 0.5866\n",
      "Testing: \tAvg loss 8516.221680,\t C index 0.5796\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Training: \tloss 137438.187500,\t C index 0.6330\n",
      "Testing: \tAvg loss 8522.566406,\t C index 0.5702\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Training: \tloss 137412.703125,\t C index 0.6290\n",
      "Testing: \tAvg loss 8536.962891,\t C index 0.5603\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Training: \tloss 135093.781250,\t C index 0.6558\n",
      "Testing: \tAvg loss 8553.620117,\t C index 0.5479\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Training: \tloss 136962.625000,\t C index 0.6522\n",
      "Testing: \tAvg loss 8566.005859,\t C index 0.5341\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Training: \tloss 132249.343750,\t C index 0.6745\n",
      "Testing: \tAvg loss 8581.777344,\t C index 0.5302\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Training: \tloss 131385.640625,\t C index 0.6992\n",
      "Testing: \tAvg loss 8581.097656,\t C index 0.5213\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Training: \tloss 130367.015625,\t C index 0.6937\n",
      "Testing: \tAvg loss 8598.748047,\t C index 0.5257\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Training: \tloss 133362.828125,\t C index 0.6794\n",
      "Testing: \tAvg loss 8616.365234,\t C index 0.5262\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Training: \tloss 127955.742188,\t C index 0.7133\n",
      "Testing: \tAvg loss 8644.888672,\t C index 0.5203\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Training: \tloss 128298.015625,\t C index 0.7025\n",
      "Testing: \tAvg loss 8667.469727,\t C index 0.5168\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Training: \tloss 127354.726562,\t C index 0.7128\n",
      "Testing: \tAvg loss 8695.533203,\t C index 0.5272\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Training: \tloss 127271.843750,\t C index 0.7180\n",
      "Testing: \tAvg loss 8745.315430,\t C index 0.5222\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Training: \tloss 127881.453125,\t C index 0.7311\n",
      "Testing: \tAvg loss 8739.593750,\t C index 0.5158\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Training: \tloss 128133.875000,\t C index 0.6973\n",
      "Testing: \tAvg loss 8780.082031,\t C index 0.5138\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Training: \tloss 121820.062500,\t C index 0.7534\n",
      "Testing: \tAvg loss 8825.603516,\t C index 0.5158\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Training: \tloss 123101.890625,\t C index 0.7449\n",
      "Testing: \tAvg loss 8899.252930,\t C index 0.5129\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Training: \tloss 121958.343750,\t C index 0.7533\n",
      "Testing: \tAvg loss 8905.384766,\t C index 0.5104\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Training: \tloss 124283.312500,\t C index 0.7343\n",
      "Testing: \tAvg loss 8955.434570,\t C index 0.5089\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Training: \tloss 122920.750000,\t C index 0.7191\n",
      "Testing: \tAvg loss 9025.986328,\t C index 0.5025\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Training: \tloss 125464.523438,\t C index 0.7231\n",
      "Testing: \tAvg loss 9111.650391,\t C index 0.5049\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Training: \tloss 121771.476562,\t C index 0.7411\n",
      "Testing: \tAvg loss 9162.511719,\t C index 0.5084\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Training: \tloss 121077.992188,\t C index 0.7502\n",
      "Testing: \tAvg loss 9238.856445,\t C index 0.5079\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Training: \tloss 121654.070312,\t C index 0.7382\n",
      "Testing: \tAvg loss 9336.786133,\t C index 0.5079\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Training: \tloss 124676.484375,\t C index 0.7262\n",
      "Testing: \tAvg loss 9379.774414,\t C index 0.5124\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Training: \tloss 117433.468750,\t C index 0.7539\n",
      "Testing: \tAvg loss 9512.388672,\t C index 0.5015\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Training: \tloss 121815.953125,\t C index 0.7312\n",
      "Testing: \tAvg loss 9629.907227,\t C index 0.5074\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Training: \tloss 120082.828125,\t C index 0.7320\n",
      "Testing: \tAvg loss 9682.810547,\t C index 0.5104\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Training: \tloss 118490.789062,\t C index 0.7408\n",
      "Testing: \tAvg loss 9760.198242,\t C index 0.5109\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Training: \tloss 116740.140625,\t C index 0.7544\n",
      "Testing: \tAvg loss 9854.570312,\t C index 0.5079\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Training: \tloss 118367.515625,\t C index 0.7463\n",
      "Testing: \tAvg loss 9861.880859,\t C index 0.5143\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Training: \tloss 116933.453125,\t C index 0.7466\n",
      "Testing: \tAvg loss 9958.279297,\t C index 0.5089\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Training: \tloss 116316.203125,\t C index 0.7554\n",
      "Testing: \tAvg loss 10003.263672,\t C index 0.5044\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Training: \tloss 115337.875000,\t C index 0.7590\n",
      "Testing: \tAvg loss 10072.562500,\t C index 0.5040\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Training: \tloss 114218.484375,\t C index 0.7647\n",
      "Testing: \tAvg loss 10120.458984,\t C index 0.5049\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Training: \tloss 116734.531250,\t C index 0.7575\n",
      "Testing: \tAvg loss 10243.749023,\t C index 0.5030\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Training: \tloss 116446.687500,\t C index 0.7525\n",
      "Testing: \tAvg loss 10254.849609,\t C index 0.5015\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Training: \tloss 117573.585938,\t C index 0.7537\n",
      "Testing: \tAvg loss 10341.103516,\t C index 0.4926\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Training: \tloss 114851.664062,\t C index 0.7578\n",
      "Testing: \tAvg loss 10444.791016,\t C index 0.4921\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Training: \tloss 114575.281250,\t C index 0.7634\n",
      "Testing: \tAvg loss 10291.552734,\t C index 0.5114\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Training: \tloss 112778.804688,\t C index 0.7643\n",
      "Testing: \tAvg loss 10383.583984,\t C index 0.5133\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Training: \tloss 117345.140625,\t C index 0.7494\n",
      "Testing: \tAvg loss 10417.016602,\t C index 0.5148\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Training: \tloss 112838.101562,\t C index 0.7677\n",
      "Testing: \tAvg loss 10497.542969,\t C index 0.5129\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Training: \tloss 113170.992188,\t C index 0.7652\n",
      "Testing: \tAvg loss 10594.509766,\t C index 0.5020\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Training: \tloss 115521.968750,\t C index 0.7554\n",
      "Testing: \tAvg loss 10686.855469,\t C index 0.5168\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Training: \tloss 112755.632812,\t C index 0.7649\n",
      "Testing: \tAvg loss 10678.182617,\t C index 0.5163\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Training: \tloss 113366.734375,\t C index 0.7648\n",
      "Testing: \tAvg loss 10767.920898,\t C index 0.5084\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Training: \tloss 112144.414062,\t C index 0.7718\n",
      "Testing: \tAvg loss 10897.783203,\t C index 0.5015\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Training: \tloss 111158.125000,\t C index 0.7691\n",
      "Testing: \tAvg loss 10953.658203,\t C index 0.5020\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Training: \tloss 110149.671875,\t C index 0.7755\n",
      "Testing: \tAvg loss 10949.713867,\t C index 0.5025\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Training: \tloss 110100.703125,\t C index 0.7736\n",
      "Testing: \tAvg loss 10831.425781,\t C index 0.5059\n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Training: \tloss 108092.632812,\t C index 0.7905\n",
      "Testing: \tAvg loss 10718.115234,\t C index 0.5030\n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Training: \tloss 111354.617188,\t C index 0.7726\n",
      "Testing: \tAvg loss 10769.804688,\t C index 0.4970\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Training: \tloss 110735.312500,\t C index 0.7718\n",
      "Testing: \tAvg loss 10744.825195,\t C index 0.5049\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Training: \tloss 108239.820312,\t C index 0.7855\n",
      "Testing: \tAvg loss 10822.312500,\t C index 0.5079\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Training: \tloss 107891.757812,\t C index 0.7831\n",
      "Testing: \tAvg loss 10912.083008,\t C index 0.5079\n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Training: \tloss 113701.710938,\t C index 0.7507\n",
      "Testing: \tAvg loss 10894.339844,\t C index 0.5054\n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Training: \tloss 107926.828125,\t C index 0.7767\n",
      "Testing: \tAvg loss 10843.595703,\t C index 0.5114\n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Training: \tloss 107320.226562,\t C index 0.7777\n",
      "Testing: \tAvg loss 10905.824219,\t C index 0.4990\n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Training: \tloss 106420.390625,\t C index 0.7808\n",
      "Testing: \tAvg loss 10945.211914,\t C index 0.5020\n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Training: \tloss 103991.445312,\t C index 0.7835\n",
      "Testing: \tAvg loss 10978.708984,\t C index 0.5040\n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Training: \tloss 104135.851562,\t C index 0.7976\n",
      "Testing: \tAvg loss 11269.944336,\t C index 0.5025\n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Training: \tloss 104892.625000,\t C index 0.7842\n",
      "Testing: \tAvg loss 11151.107422,\t C index 0.4985\n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Training: \tloss 106866.093750,\t C index 0.7621\n",
      "Testing: \tAvg loss 11117.326172,\t C index 0.4970\n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Training: \tloss 109480.640625,\t C index 0.7797\n",
      "Testing: \tAvg loss 11180.453125,\t C index 0.5025\n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Training: \tloss 110596.265625,\t C index 0.7798\n",
      "Testing: \tAvg loss 11212.912109,\t C index 0.4960\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Training: \tloss 110185.640625,\t C index 0.7772\n",
      "Testing: \tAvg loss 11115.621094,\t C index 0.5010\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Training: \tloss 110035.023438,\t C index 0.7814\n",
      "Testing: \tAvg loss 11295.212891,\t C index 0.5049\n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Training: \tloss 102124.320312,\t C index 0.7985\n",
      "Testing: \tAvg loss 11282.480469,\t C index 0.5074\n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Training: \tloss 104881.187500,\t C index 0.7741\n",
      "Testing: \tAvg loss 11354.572266,\t C index 0.5094\n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Training: \tloss 106570.625000,\t C index 0.7878\n",
      "Testing: \tAvg loss 11407.244141,\t C index 0.5094\n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Training: \tloss 106905.000000,\t C index 0.7839\n",
      "Testing: \tAvg loss 11487.347656,\t C index 0.5044\n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Training: \tloss 105811.359375,\t C index 0.7733\n",
      "Testing: \tAvg loss 11636.077148,\t C index 0.5005\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Training: \tloss 108522.507812,\t C index 0.7701\n",
      "Testing: \tAvg loss 11531.791992,\t C index 0.5089\n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Training: \tloss 105711.796875,\t C index 0.7874\n",
      "Testing: \tAvg loss 11512.279297,\t C index 0.5025\n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Training: \tloss 102889.492188,\t C index 0.7929\n",
      "Testing: \tAvg loss 11445.820312,\t C index 0.5049\n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Training: \tloss 106222.046875,\t C index 0.7934\n",
      "Testing: \tAvg loss 11428.714844,\t C index 0.5119\n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Training: \tloss 98592.429688,\t C index 0.8032\n",
      "Testing: \tAvg loss 11550.831055,\t C index 0.4995\n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Training: \tloss 103023.453125,\t C index 0.7995\n",
      "Testing: \tAvg loss 11662.039062,\t C index 0.4951\n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Training: \tloss 104693.437500,\t C index 0.7769\n",
      "Testing: \tAvg loss 11527.540039,\t C index 0.5030\n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Training: \tloss 101961.250000,\t C index 0.7878\n",
      "Testing: \tAvg loss 11379.222656,\t C index 0.5079\n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Training: \tloss 99345.023438,\t C index 0.7919\n",
      "Testing: \tAvg loss 11319.246094,\t C index 0.5153\n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Training: \tloss 102808.164062,\t C index 0.7773\n",
      "Testing: \tAvg loss 11343.593750,\t C index 0.5173\n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Training: \tloss 100304.828125,\t C index 0.7880\n",
      "Testing: \tAvg loss 11537.385742,\t C index 0.5040\n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Training: \tloss 102139.281250,\t C index 0.8064\n",
      "Testing: \tAvg loss 11601.173828,\t C index 0.5035\n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Training: \tloss 105223.507812,\t C index 0.7903\n",
      "Testing: \tAvg loss 11496.648438,\t C index 0.5089\n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Training: \tloss 100757.640625,\t C index 0.7996\n",
      "Testing: \tAvg loss 11658.341797,\t C index 0.5040\n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Training: \tloss 102674.140625,\t C index 0.7961\n",
      "Testing: \tAvg loss 11673.902344,\t C index 0.5049\n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Training: \tloss 98589.335938,\t C index 0.8031\n",
      "Testing: \tAvg loss 11571.201172,\t C index 0.5040\n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Training: \tloss 100832.828125,\t C index 0.7900\n",
      "Testing: \tAvg loss 11905.772461,\t C index 0.5040\n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Training: \tloss 101622.773438,\t C index 0.8015\n",
      "Testing: \tAvg loss 11833.164062,\t C index 0.4970\n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Training: \tloss 103213.828125,\t C index 0.8071\n",
      "Testing: \tAvg loss 11856.297852,\t C index 0.5005\n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Training: \tloss 102188.179688,\t C index 0.7919\n",
      "Testing: \tAvg loss 11883.979492,\t C index 0.4980\n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Training: \tloss 99619.343750,\t C index 0.8026\n",
      "Testing: \tAvg loss 12077.888672,\t C index 0.4980\n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Training: \tloss 96845.015625,\t C index 0.8057\n",
      "Testing: \tAvg loss 12292.437500,\t C index 0.4931\n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Training: \tloss 101681.046875,\t C index 0.8028\n",
      "Testing: \tAvg loss 12246.113281,\t C index 0.4891\n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Training: \tloss 101627.890625,\t C index 0.7837\n",
      "Testing: \tAvg loss 12242.773438,\t C index 0.4965\n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Training: \tloss 108716.859375,\t C index 0.7838\n",
      "Testing: \tAvg loss 11769.949219,\t C index 0.5040\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Training: \tloss 103602.343750,\t C index 0.7864\n",
      "Testing: \tAvg loss 12027.101562,\t C index 0.4941\n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Training: \tloss 96774.421875,\t C index 0.8021\n",
      "Testing: \tAvg loss 12173.052734,\t C index 0.5010\n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Training: \tloss 102421.109375,\t C index 0.7915\n",
      "Testing: \tAvg loss 12192.963867,\t C index 0.4960\n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Training: \tloss 101254.718750,\t C index 0.7941\n",
      "Testing: \tAvg loss 12051.988281,\t C index 0.4990\n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Training: \tloss 96770.968750,\t C index 0.8147\n",
      "Testing: \tAvg loss 12196.732422,\t C index 0.4916\n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Training: \tloss 96932.140625,\t C index 0.8017\n",
      "Testing: \tAvg loss 12361.693359,\t C index 0.4867\n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Training: \tloss 97423.343750,\t C index 0.8093\n",
      "Testing: \tAvg loss 12355.092773,\t C index 0.4901\n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Training: \tloss 95943.070312,\t C index 0.8040\n",
      "Testing: \tAvg loss 12450.205078,\t C index 0.4985\n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Training: \tloss 100896.179688,\t C index 0.7941\n",
      "Testing: \tAvg loss 12482.615234,\t C index 0.4921\n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Training: \tloss 98221.734375,\t C index 0.8009\n",
      "Testing: \tAvg loss 12337.855469,\t C index 0.4965\n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Training: \tloss 95577.359375,\t C index 0.8050\n",
      "Testing: \tAvg loss 12432.580078,\t C index 0.4965\n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Training: \tloss 96059.484375,\t C index 0.8103\n",
      "Testing: \tAvg loss 12145.155273,\t C index 0.4995\n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Training: \tloss 95931.531250,\t C index 0.8172\n",
      "Testing: \tAvg loss 12092.438477,\t C index 0.4960\n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Training: \tloss 98397.992188,\t C index 0.8003\n",
      "Testing: \tAvg loss 12036.681641,\t C index 0.5040\n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Training: \tloss 96636.421875,\t C index 0.8076\n",
      "Testing: \tAvg loss 11982.750000,\t C index 0.5064\n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Training: \tloss 101320.500000,\t C index 0.8080\n",
      "Testing: \tAvg loss 12065.447266,\t C index 0.5074\n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Training: \tloss 89279.937500,\t C index 0.8197\n",
      "Testing: \tAvg loss 12226.811523,\t C index 0.5059\n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Training: \tloss 95019.781250,\t C index 0.8055\n",
      "Testing: \tAvg loss 12325.062500,\t C index 0.5005\n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Training: \tloss 93597.890625,\t C index 0.8148\n",
      "Testing: \tAvg loss 12199.153320,\t C index 0.5035\n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Training: \tloss 102074.468750,\t C index 0.7882\n",
      "Testing: \tAvg loss 12202.533203,\t C index 0.5059\n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Training: \tloss 93574.203125,\t C index 0.8105\n",
      "Testing: \tAvg loss 11969.352539,\t C index 0.5148\n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Training: \tloss 90324.265625,\t C index 0.8329\n",
      "Testing: \tAvg loss 12543.832031,\t C index 0.5074\n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Training: \tloss 98740.250000,\t C index 0.7971\n",
      "Testing: \tAvg loss 12361.532227,\t C index 0.5124\n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Training: \tloss 92325.671875,\t C index 0.8023\n",
      "Testing: \tAvg loss 12426.552734,\t C index 0.5183\n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Training: \tloss 94634.523438,\t C index 0.8212\n",
      "Testing: \tAvg loss 12591.931641,\t C index 0.5099\n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Training: \tloss 92345.812500,\t C index 0.8013\n",
      "Testing: \tAvg loss 12590.972656,\t C index 0.5099\n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Training: \tloss 93086.937500,\t C index 0.8172\n",
      "Testing: \tAvg loss 12645.324219,\t C index 0.5069\n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Training: \tloss 98134.039062,\t C index 0.8082\n",
      "Testing: \tAvg loss 13134.962891,\t C index 0.5129\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Training: \tloss 88438.593750,\t C index 0.8195\n",
      "Testing: \tAvg loss 13104.417969,\t C index 0.5035\n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Training: \tloss 94189.164062,\t C index 0.8179\n",
      "Testing: \tAvg loss 12985.455078,\t C index 0.5040\n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Training: \tloss 102099.351562,\t C index 0.7990\n",
      "Testing: \tAvg loss 12766.621094,\t C index 0.5138\n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Training: \tloss 96850.570312,\t C index 0.8097\n",
      "Testing: \tAvg loss 12796.844727,\t C index 0.5089\n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Training: \tloss 90779.937500,\t C index 0.8117\n",
      "Testing: \tAvg loss 12511.840820,\t C index 0.5153\n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Training: \tloss 104522.828125,\t C index 0.7959\n",
      "Testing: \tAvg loss 13028.302734,\t C index 0.5005\n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Training: \tloss 91546.664062,\t C index 0.8198\n",
      "Testing: \tAvg loss 13301.670898,\t C index 0.4985\n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Training: \tloss 97138.750000,\t C index 0.8007\n",
      "Testing: \tAvg loss 13555.720703,\t C index 0.5005\n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Training: \tloss 92939.875000,\t C index 0.8176\n",
      "Testing: \tAvg loss 13523.611328,\t C index 0.5020\n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Training: \tloss 92680.976562,\t C index 0.8215\n",
      "Testing: \tAvg loss 13474.609375,\t C index 0.4946\n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Training: \tloss 90856.460938,\t C index 0.8250\n",
      "Testing: \tAvg loss 13311.898438,\t C index 0.4951\n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Training: \tloss 96655.046875,\t C index 0.8095\n",
      "Testing: \tAvg loss 13172.423828,\t C index 0.5005\n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Training: \tloss 96162.265625,\t C index 0.8212\n",
      "Testing: \tAvg loss 13089.723633,\t C index 0.5000\n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Training: \tloss 92006.781250,\t C index 0.8174\n",
      "Testing: \tAvg loss 12752.763672,\t C index 0.5193\n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Training: \tloss 95449.921875,\t C index 0.8123\n",
      "Testing: \tAvg loss 13342.127930,\t C index 0.5133\n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Training: \tloss 93536.101562,\t C index 0.8031\n",
      "Testing: \tAvg loss 13027.525391,\t C index 0.5148\n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Training: \tloss 91965.031250,\t C index 0.8177\n",
      "Testing: \tAvg loss 13117.132812,\t C index 0.5079\n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Training: \tloss 90437.328125,\t C index 0.8164\n",
      "Testing: \tAvg loss 13425.270508,\t C index 0.5069\n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Training: \tloss 94209.960938,\t C index 0.8155\n",
      "Testing: \tAvg loss 13515.088867,\t C index 0.5049\n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Training: \tloss 90345.867188,\t C index 0.8150\n",
      "Testing: \tAvg loss 13519.341797,\t C index 0.5099\n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Training: \tloss 88911.648438,\t C index 0.8154\n",
      "Testing: \tAvg loss 13626.016602,\t C index 0.5059\n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Training: \tloss 90817.484375,\t C index 0.8092\n",
      "Testing: \tAvg loss 13273.682617,\t C index 0.5138\n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Training: \tloss 90748.437500,\t C index 0.8177\n",
      "Testing: \tAvg loss 12995.394531,\t C index 0.5143\n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Training: \tloss 93140.640625,\t C index 0.8175\n",
      "Testing: \tAvg loss 13281.320312,\t C index 0.5124\n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Training: \tloss 89597.265625,\t C index 0.8108\n",
      "Testing: \tAvg loss 12889.415039,\t C index 0.5178\n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Training: \tloss 90706.343750,\t C index 0.8130\n",
      "Testing: \tAvg loss 13070.929688,\t C index 0.5178\n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Training: \tloss 87114.718750,\t C index 0.8343\n",
      "Testing: \tAvg loss 13043.621094,\t C index 0.5158\n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Training: \tloss 98070.414062,\t C index 0.8224\n",
      "Testing: \tAvg loss 13334.074219,\t C index 0.5163\n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Training: \tloss 87270.296875,\t C index 0.8230\n",
      "Testing: \tAvg loss 13540.505859,\t C index 0.5074\n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Training: \tloss 89117.390625,\t C index 0.8186\n",
      "Testing: \tAvg loss 13471.637695,\t C index 0.5114\n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Training: \tloss 88154.203125,\t C index 0.8223\n",
      "Testing: \tAvg loss 13633.046875,\t C index 0.5094\n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Training: \tloss 88533.421875,\t C index 0.8156\n",
      "Testing: \tAvg loss 13451.644531,\t C index 0.5074\n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Training: \tloss 92648.203125,\t C index 0.8135\n",
      "Testing: \tAvg loss 13412.283203,\t C index 0.5148\n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Training: \tloss 86190.929688,\t C index 0.8316\n",
      "Testing: \tAvg loss 13359.013672,\t C index 0.5119\n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Training: \tloss 91172.578125,\t C index 0.8057\n",
      "Testing: \tAvg loss 13411.946289,\t C index 0.5104\n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Training: \tloss 84453.632812,\t C index 0.8337\n",
      "Testing: \tAvg loss 13234.238281,\t C index 0.5133\n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Training: \tloss 92302.890625,\t C index 0.8116\n",
      "Testing: \tAvg loss 12986.909180,\t C index 0.5188\n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Training: \tloss 87972.656250,\t C index 0.8251\n",
      "Testing: \tAvg loss 13257.294922,\t C index 0.5148\n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Training: \tloss 86400.359375,\t C index 0.8200\n",
      "Testing: \tAvg loss 13378.630859,\t C index 0.5163\n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Training: \tloss 89581.617188,\t C index 0.8066\n",
      "Testing: \tAvg loss 13454.791016,\t C index 0.5158\n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Training: \tloss 84460.257812,\t C index 0.8226\n",
      "Testing: \tAvg loss 13433.697266,\t C index 0.5173\n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Training: \tloss 87062.468750,\t C index 0.8272\n",
      "Testing: \tAvg loss 13156.496094,\t C index 0.5203\n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Training: \tloss 90239.562500,\t C index 0.8413\n",
      "Testing: \tAvg loss 13354.638672,\t C index 0.5237\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Training: \tloss 84752.062500,\t C index 0.8321\n",
      "Testing: \tAvg loss 13329.912109,\t C index 0.5168\n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Training: \tloss 89267.328125,\t C index 0.8189\n",
      "Testing: \tAvg loss 13291.447266,\t C index 0.5183\n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Training: \tloss 87929.812500,\t C index 0.8239\n",
      "Testing: \tAvg loss 13521.602539,\t C index 0.5198\n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Training: \tloss 83941.203125,\t C index 0.8289\n",
      "Testing: \tAvg loss 13743.053711,\t C index 0.5267\n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Training: \tloss 90397.921875,\t C index 0.8142\n",
      "Testing: \tAvg loss 13214.247070,\t C index 0.5222\n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Training: \tloss 91093.328125,\t C index 0.8304\n",
      "Testing: \tAvg loss 13432.625000,\t C index 0.5198\n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Training: \tloss 88355.945312,\t C index 0.8315\n",
      "Testing: \tAvg loss 13283.335938,\t C index 0.5292\n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Training: \tloss 91558.226562,\t C index 0.8235\n",
      "Testing: \tAvg loss 13681.867188,\t C index 0.5198\n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Training: \tloss 80403.460938,\t C index 0.8380\n",
      "Testing: \tAvg loss 14084.031250,\t C index 0.5168\n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Training: \tloss 85526.656250,\t C index 0.8327\n",
      "Testing: \tAvg loss 14373.238281,\t C index 0.5217\n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Training: \tloss 84946.625000,\t C index 0.8282\n",
      "Testing: \tAvg loss 14629.516602,\t C index 0.5079\n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Training: \tloss 85477.296875,\t C index 0.8314\n",
      "Testing: \tAvg loss 14534.833008,\t C index 0.5109\n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Training: \tloss 79481.476562,\t C index 0.8279\n",
      "Testing: \tAvg loss 14074.847656,\t C index 0.5198\n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Training: \tloss 87206.109375,\t C index 0.8309\n",
      "Testing: \tAvg loss 14327.368164,\t C index 0.5168\n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Training: \tloss 83402.914062,\t C index 0.8405\n",
      "Testing: \tAvg loss 14524.716797,\t C index 0.5129\n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Training: \tloss 82856.546875,\t C index 0.8327\n",
      "Testing: \tAvg loss 14638.322266,\t C index 0.5114\n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Training: \tloss 84607.203125,\t C index 0.8229\n",
      "Testing: \tAvg loss 14695.540039,\t C index 0.5124\n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Training: \tloss 83817.273438,\t C index 0.8298\n",
      "Testing: \tAvg loss 14734.605469,\t C index 0.5183\n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Training: \tloss 86877.867188,\t C index 0.8286\n",
      "Testing: \tAvg loss 15272.441406,\t C index 0.5124\n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Training: \tloss 86505.343750,\t C index 0.8139\n",
      "Testing: \tAvg loss 14717.605469,\t C index 0.5163\n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Training: \tloss 90124.000000,\t C index 0.8191\n",
      "Testing: \tAvg loss 14515.705078,\t C index 0.5173\n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Training: \tloss 79073.890625,\t C index 0.8332\n",
      "Testing: \tAvg loss 14432.783203,\t C index 0.5148\n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Training: \tloss 85064.437500,\t C index 0.8337\n",
      "Testing: \tAvg loss 14860.439453,\t C index 0.5208\n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Training: \tloss 83243.421875,\t C index 0.8275\n",
      "Testing: \tAvg loss 14240.957031,\t C index 0.5237\n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Training: \tloss 94136.296875,\t C index 0.8247\n",
      "Testing: \tAvg loss 14305.468750,\t C index 0.5198\n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Training: \tloss 83870.242188,\t C index 0.8286\n",
      "Testing: \tAvg loss 14499.556641,\t C index 0.5208\n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Training: \tloss 86885.281250,\t C index 0.8195\n",
      "Testing: \tAvg loss 14869.433594,\t C index 0.5183\n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Training: \tloss 83785.062500,\t C index 0.8309\n",
      "Testing: \tAvg loss 15021.992188,\t C index 0.5227\n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Training: \tloss 82818.375000,\t C index 0.8284\n",
      "Testing: \tAvg loss 15029.252930,\t C index 0.5183\n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Training: \tloss 82390.734375,\t C index 0.8245\n",
      "Testing: \tAvg loss 14909.894531,\t C index 0.5203\n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Training: \tloss 89405.390625,\t C index 0.8302\n",
      "Testing: \tAvg loss 15144.990234,\t C index 0.5119\n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Training: \tloss 79134.140625,\t C index 0.8308\n",
      "Testing: \tAvg loss 15302.990234,\t C index 0.5138\n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Training: \tloss 85397.703125,\t C index 0.8203\n",
      "Testing: \tAvg loss 15151.040039,\t C index 0.5188\n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Training: \tloss 82911.468750,\t C index 0.8313\n",
      "Testing: \tAvg loss 15384.366211,\t C index 0.5178\n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Training: \tloss 81330.195312,\t C index 0.8418\n",
      "Testing: \tAvg loss 15103.230469,\t C index 0.5257\n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Training: \tloss 76331.070312,\t C index 0.8385\n",
      "Testing: \tAvg loss 14512.462891,\t C index 0.5277\n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Training: \tloss 87430.609375,\t C index 0.8303\n",
      "Testing: \tAvg loss 14634.940430,\t C index 0.5237\n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Training: \tloss 82354.312500,\t C index 0.8300\n",
      "Testing: \tAvg loss 14672.416016,\t C index 0.5188\n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Training: \tloss 82192.976562,\t C index 0.8392\n",
      "Testing: \tAvg loss 15306.383789,\t C index 0.5069\n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Training: \tloss 87367.625000,\t C index 0.8178\n",
      "Testing: \tAvg loss 15780.556641,\t C index 0.5104\n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Training: \tloss 78059.687500,\t C index 0.8360\n",
      "Testing: \tAvg loss 15461.117188,\t C index 0.5143\n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Training: \tloss 76859.328125,\t C index 0.8413\n",
      "Testing: \tAvg loss 15392.853516,\t C index 0.5054\n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Training: \tloss 80043.531250,\t C index 0.8259\n",
      "Testing: \tAvg loss 15139.016602,\t C index 0.5079\n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Training: \tloss 75818.453125,\t C index 0.8403\n",
      "Testing: \tAvg loss 15260.065430,\t C index 0.5069\n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Training: \tloss 77499.429688,\t C index 0.8282\n",
      "Testing: \tAvg loss 15054.056641,\t C index 0.5089\n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Training: \tloss 79935.421875,\t C index 0.8322\n",
      "Testing: \tAvg loss 14965.295898,\t C index 0.5129\n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Training: \tloss 80174.125000,\t C index 0.8384\n",
      "Testing: \tAvg loss 15461.590820,\t C index 0.5059\n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Training: \tloss 80690.046875,\t C index 0.8392\n",
      "Testing: \tAvg loss 15646.705078,\t C index 0.4990\n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Training: \tloss 85831.070312,\t C index 0.8312\n",
      "Testing: \tAvg loss 15206.859375,\t C index 0.5153\n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Training: \tloss 78702.234375,\t C index 0.8475\n",
      "Testing: \tAvg loss 15295.324219,\t C index 0.5237\n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Training: \tloss 73318.593750,\t C index 0.8545\n",
      "Testing: \tAvg loss 15290.214844,\t C index 0.5262\n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Training: \tloss 81084.851562,\t C index 0.8400\n",
      "Testing: \tAvg loss 15359.878906,\t C index 0.5198\n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Training: \tloss 74427.437500,\t C index 0.8469\n",
      "Testing: \tAvg loss 15595.055664,\t C index 0.5148\n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Training: \tloss 82484.718750,\t C index 0.8316\n",
      "Testing: \tAvg loss 15616.878906,\t C index 0.5079\n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Training: \tloss 75932.656250,\t C index 0.8417\n",
      "Testing: \tAvg loss 15626.031250,\t C index 0.5114\n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Training: \tloss 85871.812500,\t C index 0.8305\n",
      "Testing: \tAvg loss 16299.617188,\t C index 0.5099\n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Training: \tloss 83007.687500,\t C index 0.8300\n",
      "Testing: \tAvg loss 16260.567383,\t C index 0.5040\n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Training: \tloss 85864.398438,\t C index 0.8212\n",
      "Testing: \tAvg loss 15427.309570,\t C index 0.5158\n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Training: \tloss 79536.632812,\t C index 0.8320\n",
      "Testing: \tAvg loss 14856.859375,\t C index 0.5143\n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Training: \tloss 81227.093750,\t C index 0.8263\n",
      "Testing: \tAvg loss 14930.816406,\t C index 0.5193\n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Training: \tloss 78768.406250,\t C index 0.8426\n",
      "Testing: \tAvg loss 15310.881836,\t C index 0.5213\n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Training: \tloss 96544.281250,\t C index 0.8339\n",
      "Testing: \tAvg loss 16866.519531,\t C index 0.5049\n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Training: \tloss 85510.210938,\t C index 0.8249\n",
      "Testing: \tAvg loss 16083.794922,\t C index 0.5099\n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Training: \tloss 81766.171875,\t C index 0.8303\n",
      "Testing: \tAvg loss 15593.533203,\t C index 0.5124\n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Training: \tloss 78932.609375,\t C index 0.8314\n",
      "Testing: \tAvg loss 16058.412109,\t C index 0.5104\n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Training: \tloss 77408.484375,\t C index 0.8348\n",
      "Testing: \tAvg loss 15637.589844,\t C index 0.5099\n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Training: \tloss 72915.468750,\t C index 0.8431\n",
      "Testing: \tAvg loss 15794.524414,\t C index 0.5198\n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Training: \tloss 84260.109375,\t C index 0.8381\n",
      "Testing: \tAvg loss 15969.883789,\t C index 0.5138\n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Training: \tloss 75006.171875,\t C index 0.8470\n",
      "Testing: \tAvg loss 15955.482422,\t C index 0.5168\n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Training: \tloss 75383.906250,\t C index 0.8473\n",
      "Testing: \tAvg loss 16376.051758,\t C index 0.5198\n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Training: \tloss 78033.265625,\t C index 0.8298\n",
      "Testing: \tAvg loss 16498.093750,\t C index 0.5163\n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Training: \tloss 84721.171875,\t C index 0.8266\n",
      "Testing: \tAvg loss 15967.773438,\t C index 0.5217\n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Training: \tloss 85640.000000,\t C index 0.8229\n",
      "Testing: \tAvg loss 15300.791016,\t C index 0.5277\n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Training: \tloss 75288.812500,\t C index 0.8432\n",
      "Testing: \tAvg loss 15357.333984,\t C index 0.5213\n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Training: \tloss 78646.156250,\t C index 0.8333\n",
      "Testing: \tAvg loss 15727.195312,\t C index 0.5183\n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Training: \tloss 71378.851562,\t C index 0.8452\n",
      "Testing: \tAvg loss 15526.072266,\t C index 0.5262\n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Training: \tloss 75960.906250,\t C index 0.8488\n",
      "Testing: \tAvg loss 15587.532227,\t C index 0.5173\n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Training: \tloss 73714.843750,\t C index 0.8404\n",
      "Testing: \tAvg loss 15678.136719,\t C index 0.5104\n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Training: \tloss 75243.265625,\t C index 0.8477\n",
      "Testing: \tAvg loss 15299.843750,\t C index 0.5143\n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Training: \tloss 77751.406250,\t C index 0.8405\n",
      "Testing: \tAvg loss 15994.001953,\t C index 0.5138\n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Training: \tloss 75985.203125,\t C index 0.8264\n",
      "Testing: \tAvg loss 15461.375000,\t C index 0.5133\n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Training: \tloss 70606.937500,\t C index 0.8418\n",
      "Testing: \tAvg loss 15366.054688,\t C index 0.5168\n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Training: \tloss 77591.859375,\t C index 0.8432\n",
      "Testing: \tAvg loss 16043.666016,\t C index 0.5099\n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Training: \tloss 75642.203125,\t C index 0.8345\n",
      "Testing: \tAvg loss 16117.703125,\t C index 0.5213\n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Training: \tloss 77774.296875,\t C index 0.8298\n",
      "Testing: \tAvg loss 16127.371094,\t C index 0.5193\n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Training: \tloss 75699.593750,\t C index 0.8413\n",
      "Testing: \tAvg loss 15820.389648,\t C index 0.5193\n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Training: \tloss 81353.765625,\t C index 0.8345\n",
      "Testing: \tAvg loss 16599.023438,\t C index 0.5015\n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Training: \tloss 83895.078125,\t C index 0.8331\n",
      "Testing: \tAvg loss 17187.136719,\t C index 0.5084\n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Training: \tloss 74345.953125,\t C index 0.8397\n",
      "Testing: \tAvg loss 17235.761719,\t C index 0.5143\n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Training: \tloss 70390.945312,\t C index 0.8460\n",
      "Testing: \tAvg loss 17696.119141,\t C index 0.4990\n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Training: \tloss 77036.218750,\t C index 0.8375\n",
      "Testing: \tAvg loss 17652.640625,\t C index 0.4990\n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Training: \tloss 84213.671875,\t C index 0.8250\n",
      "Testing: \tAvg loss 18079.097656,\t C index 0.5104\n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Training: \tloss 75161.015625,\t C index 0.8311\n",
      "Testing: \tAvg loss 17841.859375,\t C index 0.5074\n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Training: \tloss 74063.570312,\t C index 0.8363\n",
      "Testing: \tAvg loss 16745.832031,\t C index 0.5138\n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Training: \tloss 69579.406250,\t C index 0.8421\n",
      "Testing: \tAvg loss 17059.609375,\t C index 0.5158\n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Training: \tloss 80967.015625,\t C index 0.8228\n",
      "Testing: \tAvg loss 16801.738281,\t C index 0.5129\n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Training: \tloss 77884.500000,\t C index 0.8260\n",
      "Testing: \tAvg loss 16807.357422,\t C index 0.5163\n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Training: \tloss 76041.523438,\t C index 0.8246\n",
      "Testing: \tAvg loss 15837.989258,\t C index 0.5163\n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Training: \tloss 75759.968750,\t C index 0.8276\n",
      "Testing: \tAvg loss 16295.812500,\t C index 0.5133\n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Training: \tloss 79445.984375,\t C index 0.8246\n",
      "Testing: \tAvg loss 16410.093750,\t C index 0.5222\n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Training: \tloss 73708.070312,\t C index 0.8480\n",
      "Testing: \tAvg loss 17182.128906,\t C index 0.5153\n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Training: \tloss 72855.945312,\t C index 0.8490\n",
      "Testing: \tAvg loss 16781.689453,\t C index 0.5148\n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Training: \tloss 72352.421875,\t C index 0.8460\n",
      "Testing: \tAvg loss 17236.158203,\t C index 0.5158\n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Training: \tloss 76801.953125,\t C index 0.8387\n",
      "Testing: \tAvg loss 17862.669922,\t C index 0.5193\n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Training: \tloss 75572.078125,\t C index 0.8340\n",
      "Testing: \tAvg loss 17952.640625,\t C index 0.5153\n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Training: \tloss 68578.687500,\t C index 0.8395\n",
      "Testing: \tAvg loss 18030.966797,\t C index 0.5094\n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Training: \tloss 75991.156250,\t C index 0.8427\n",
      "Testing: \tAvg loss 17969.880859,\t C index 0.5069\n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Training: \tloss 73693.000000,\t C index 0.8380\n",
      "Testing: \tAvg loss 18076.187500,\t C index 0.5153\n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Training: \tloss 69776.421875,\t C index 0.8470\n",
      "Testing: \tAvg loss 18133.187500,\t C index 0.5163\n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Training: \tloss 81869.750000,\t C index 0.8320\n",
      "Testing: \tAvg loss 18084.800781,\t C index 0.5153\n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Training: \tloss 74602.179688,\t C index 0.8503\n",
      "Testing: \tAvg loss 18028.300781,\t C index 0.5227\n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Training: \tloss 72133.109375,\t C index 0.8532\n",
      "Testing: \tAvg loss 18360.906250,\t C index 0.5163\n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Training: \tloss 72118.656250,\t C index 0.8472\n",
      "Testing: \tAvg loss 17870.910156,\t C index 0.5183\n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Training: \tloss 67366.742188,\t C index 0.8473\n",
      "Testing: \tAvg loss 18073.263672,\t C index 0.5148\n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Training: \tloss 63517.921875,\t C index 0.8587\n",
      "Testing: \tAvg loss 18092.015625,\t C index 0.5153\n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Training: \tloss 67310.304688,\t C index 0.8460\n",
      "Testing: \tAvg loss 18569.941406,\t C index 0.5099\n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Training: \tloss 76509.257812,\t C index 0.8384\n",
      "Testing: \tAvg loss 19571.332031,\t C index 0.5079\n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Training: \tloss 68658.328125,\t C index 0.8480\n",
      "Testing: \tAvg loss 19097.490234,\t C index 0.5188\n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Training: \tloss 79128.664062,\t C index 0.8383\n",
      "Testing: \tAvg loss 19056.091797,\t C index 0.5168\n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Training: \tloss 68679.718750,\t C index 0.8422\n",
      "Testing: \tAvg loss 18989.900391,\t C index 0.5262\n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Training: \tloss 58757.902344,\t C index 0.8567\n",
      "Testing: \tAvg loss 18515.154297,\t C index 0.5217\n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Training: \tloss 72281.062500,\t C index 0.8536\n",
      "Testing: \tAvg loss 18739.820312,\t C index 0.5158\n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Training: \tloss 75356.929688,\t C index 0.8443\n",
      "Testing: \tAvg loss 19378.585938,\t C index 0.5151\n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Training: \tloss 77110.375000,\t C index 0.8391\n",
      "Testing: \tAvg loss 18466.976562,\t C index 0.5200\n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Training: \tloss 67118.984375,\t C index 0.8523\n",
      "Testing: \tAvg loss 18134.242188,\t C index 0.5183\n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Training: \tloss 65270.656250,\t C index 0.8431\n",
      "Testing: \tAvg loss 18165.152344,\t C index 0.5262\n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Training: \tloss 72871.328125,\t C index 0.8458\n",
      "Testing: \tAvg loss 17649.734375,\t C index 0.5168\n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Training: \tloss 90740.187500,\t C index 0.8307\n",
      "Testing: \tAvg loss 17557.789062,\t C index 0.5138\n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Training: \tloss 77824.226562,\t C index 0.8504\n",
      "Testing: \tAvg loss 18328.480469,\t C index 0.5124\n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Training: \tloss 78111.421875,\t C index 0.8267\n",
      "Testing: \tAvg loss 18671.992188,\t C index 0.5074\n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Training: \tloss 77571.125000,\t C index 0.8300\n",
      "Testing: \tAvg loss 18982.847656,\t C index 0.5099\n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Training: \tloss 76237.671875,\t C index 0.8275\n",
      "Testing: \tAvg loss 18492.441406,\t C index 0.5109\n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Training: \tloss 64647.710938,\t C index 0.8488\n",
      "Testing: \tAvg loss 18460.617188,\t C index 0.5074\n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Training: \tloss 69737.171875,\t C index 0.8514\n",
      "Testing: \tAvg loss 18957.609375,\t C index 0.5069\n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Training: \tloss 68582.226562,\t C index 0.8390\n",
      "Testing: \tAvg loss 18526.642578,\t C index 0.5044\n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Training: \tloss 73187.921875,\t C index 0.8415\n",
      "Testing: \tAvg loss 19201.121094,\t C index 0.5059\n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Training: \tloss 68297.562500,\t C index 0.8457\n",
      "Testing: \tAvg loss 18587.681641,\t C index 0.5143\n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Training: \tloss 69345.703125,\t C index 0.8478\n",
      "Testing: \tAvg loss 18879.462891,\t C index 0.5069\n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Training: \tloss 71326.570312,\t C index 0.8537\n",
      "Testing: \tAvg loss 19502.023438,\t C index 0.4975\n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Training: \tloss 76083.000000,\t C index 0.8468\n",
      "Testing: \tAvg loss 19353.542969,\t C index 0.5129\n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Training: \tloss 68376.859375,\t C index 0.8486\n",
      "Testing: \tAvg loss 19215.457031,\t C index 0.5183\n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Training: \tloss 70387.281250,\t C index 0.8433\n",
      "Testing: \tAvg loss 18995.843750,\t C index 0.5217\n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Training: \tloss 71872.820312,\t C index 0.8539\n",
      "Testing: \tAvg loss 18914.736328,\t C index 0.5213\n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Training: \tloss 63718.984375,\t C index 0.8606\n",
      "Testing: \tAvg loss 18422.017578,\t C index 0.5267\n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Training: \tloss 69682.203125,\t C index 0.8506\n",
      "Testing: \tAvg loss 18168.048828,\t C index 0.5262\n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Training: \tloss 58198.531250,\t C index 0.8652\n",
      "Testing: \tAvg loss 18966.599609,\t C index 0.5203\n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Training: \tloss 74965.250000,\t C index 0.8503\n",
      "Testing: \tAvg loss 18851.621094,\t C index 0.5173\n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Training: \tloss 65705.812500,\t C index 0.8504\n",
      "Testing: \tAvg loss 19678.214844,\t C index 0.5198\n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Training: \tloss 63398.390625,\t C index 0.8576\n",
      "Testing: \tAvg loss 19226.916016,\t C index 0.5203\n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Training: \tloss 73925.468750,\t C index 0.8493\n",
      "Testing: \tAvg loss 19357.890625,\t C index 0.5217\n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Training: \tloss 62497.160156,\t C index 0.8544\n",
      "Testing: \tAvg loss 19415.898438,\t C index 0.5193\n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Training: \tloss 68051.921875,\t C index 0.8445\n",
      "Testing: \tAvg loss 19793.687500,\t C index 0.5232\n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Training: \tloss 61614.933594,\t C index 0.8581\n",
      "Testing: \tAvg loss 20727.242188,\t C index 0.5153\n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Training: \tloss 75741.562500,\t C index 0.8347\n",
      "Testing: \tAvg loss 20340.921875,\t C index 0.5148\n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Training: \tloss 64686.363281,\t C index 0.8549\n",
      "Testing: \tAvg loss 20125.246094,\t C index 0.5138\n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Training: \tloss 77328.875000,\t C index 0.8454\n",
      "Testing: \tAvg loss 20118.109375,\t C index 0.5173\n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Training: \tloss 57022.875000,\t C index 0.8626\n",
      "Testing: \tAvg loss 20509.685547,\t C index 0.5188\n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Training: \tloss 67701.015625,\t C index 0.8477\n",
      "Testing: \tAvg loss 19765.683594,\t C index 0.5198\n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Training: \tloss 67170.820312,\t C index 0.8492\n",
      "Testing: \tAvg loss 20361.671875,\t C index 0.5143\n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Training: \tloss 64112.574219,\t C index 0.8425\n",
      "Testing: \tAvg loss 20650.093750,\t C index 0.5173\n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Training: \tloss 68181.203125,\t C index 0.8514\n",
      "Testing: \tAvg loss 20503.955078,\t C index 0.5133\n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Training: \tloss 64023.328125,\t C index 0.8506\n",
      "Testing: \tAvg loss 19624.968750,\t C index 0.5163\n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Training: \tloss 70687.312500,\t C index 0.8377\n",
      "Testing: \tAvg loss 20591.523438,\t C index 0.5148\n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Training: \tloss 64759.222656,\t C index 0.8502\n",
      "Testing: \tAvg loss 19747.605469,\t C index 0.5183\n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Training: \tloss 77186.890625,\t C index 0.8321\n",
      "Testing: \tAvg loss 19765.947266,\t C index 0.5257\n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Training: \tloss 75467.984375,\t C index 0.8486\n",
      "Testing: \tAvg loss 19337.933594,\t C index 0.5208\n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Training: \tloss 59635.417969,\t C index 0.8575\n",
      "Testing: \tAvg loss 19182.757812,\t C index 0.5183\n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Training: \tloss 70097.578125,\t C index 0.8516\n",
      "Testing: \tAvg loss 19306.609375,\t C index 0.5193\n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Training: \tloss 65592.234375,\t C index 0.8408\n",
      "Testing: \tAvg loss 20345.789062,\t C index 0.5208\n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Training: \tloss 66804.187500,\t C index 0.8476\n",
      "Testing: \tAvg loss 21098.359375,\t C index 0.5119\n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Training: \tloss 75043.656250,\t C index 0.8414\n",
      "Testing: \tAvg loss 20350.087891,\t C index 0.5133\n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Training: \tloss 64661.187500,\t C index 0.8420\n",
      "Testing: \tAvg loss 20321.464844,\t C index 0.5143\n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Training: \tloss 69720.617188,\t C index 0.8511\n",
      "Testing: \tAvg loss 19561.937500,\t C index 0.5148\n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Training: \tloss 63695.746094,\t C index 0.8585\n",
      "Testing: \tAvg loss 19092.050781,\t C index 0.5168\n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Training: \tloss 61786.343750,\t C index 0.8599\n",
      "Testing: \tAvg loss 19485.878906,\t C index 0.5183\n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Training: \tloss 73641.343750,\t C index 0.8472\n",
      "Testing: \tAvg loss 19309.960938,\t C index 0.5173\n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Training: \tloss 71009.171875,\t C index 0.8402\n",
      "Testing: \tAvg loss 20090.742188,\t C index 0.5064\n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Training: \tloss 72462.375000,\t C index 0.8580\n",
      "Testing: \tAvg loss 19352.199219,\t C index 0.5119\n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Training: \tloss 73329.765625,\t C index 0.8501\n",
      "Testing: \tAvg loss 19196.738281,\t C index 0.5143\n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Training: \tloss 74896.203125,\t C index 0.8452\n",
      "Testing: \tAvg loss 18824.681641,\t C index 0.5163\n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Training: \tloss 64088.617188,\t C index 0.8564\n",
      "Testing: \tAvg loss 18568.679688,\t C index 0.5143\n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Training: \tloss 72731.000000,\t C index 0.8562\n",
      "Testing: \tAvg loss 19822.820312,\t C index 0.5069\n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Training: \tloss 65608.984375,\t C index 0.8425\n",
      "Testing: \tAvg loss 19752.132812,\t C index 0.5109\n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Training: \tloss 68724.250000,\t C index 0.8542\n",
      "Testing: \tAvg loss 18645.707031,\t C index 0.5129\n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Training: \tloss 61167.421875,\t C index 0.8684\n",
      "Testing: \tAvg loss 19456.593750,\t C index 0.5163\n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Training: \tloss 66655.515625,\t C index 0.8462\n",
      "Testing: \tAvg loss 20229.761719,\t C index 0.5025\n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Training: \tloss 66702.718750,\t C index 0.8477\n",
      "Testing: \tAvg loss 20108.876953,\t C index 0.5099\n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Training: \tloss 57083.097656,\t C index 0.8547\n",
      "Testing: \tAvg loss 19295.769531,\t C index 0.5109\n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Training: \tloss 68698.078125,\t C index 0.8508\n",
      "Testing: \tAvg loss 19390.570312,\t C index 0.5168\n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Training: \tloss 66563.015625,\t C index 0.8493\n",
      "Testing: \tAvg loss 19879.812500,\t C index 0.5064\n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Training: \tloss 67560.953125,\t C index 0.8504\n",
      "Testing: \tAvg loss 20624.109375,\t C index 0.5084\n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Training: \tloss 69718.546875,\t C index 0.8447\n",
      "Testing: \tAvg loss 20386.480469,\t C index 0.5111\n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Training: \tloss 64338.789062,\t C index 0.8531\n",
      "Testing: \tAvg loss 21535.710938,\t C index 0.4864\n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Training: \tloss 60184.839844,\t C index 0.8479\n",
      "Testing: \tAvg loss 21228.236328,\t C index 0.4948\n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Training: \tloss 59678.210938,\t C index 0.8539\n",
      "Testing: \tAvg loss 21985.652344,\t C index 0.4943\n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Training: \tloss 66894.156250,\t C index 0.8536\n",
      "Testing: \tAvg loss 21471.855469,\t C index 0.4993\n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Training: \tloss 63752.312500,\t C index 0.8377\n",
      "Testing: \tAvg loss 21078.714844,\t C index 0.4998\n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Training: \tloss 62483.875000,\t C index 0.8516\n",
      "Testing: \tAvg loss 21155.152344,\t C index 0.5007\n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Training: \tloss 70911.289062,\t C index 0.8448\n",
      "Testing: \tAvg loss 20730.732422,\t C index 0.5054\n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Training: \tloss 66532.218750,\t C index 0.8464\n",
      "Testing: \tAvg loss 19792.492188,\t C index 0.5104\n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Training: \tloss 61505.296875,\t C index 0.8585\n",
      "Testing: \tAvg loss 20134.001953,\t C index 0.5074\n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Training: \tloss 67400.687500,\t C index 0.8493\n",
      "Testing: \tAvg loss 19747.976562,\t C index 0.5049\n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Training: \tloss 58909.507812,\t C index 0.8493\n",
      "Testing: \tAvg loss 19117.072266,\t C index 0.5099\n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Training: \tloss 70160.015625,\t C index 0.8424\n",
      "Testing: \tAvg loss 19066.539062,\t C index 0.5153\n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Training: \tloss 62134.109375,\t C index 0.8532\n",
      "Testing: \tAvg loss 19603.744141,\t C index 0.5094\n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Training: \tloss 58981.507812,\t C index 0.8566\n",
      "Testing: \tAvg loss 19513.472656,\t C index 0.5252\n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Training: \tloss 71423.015625,\t C index 0.8571\n",
      "Testing: \tAvg loss 20846.667969,\t C index 0.5193\n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Training: \tloss 63723.066406,\t C index 0.8487\n",
      "Testing: \tAvg loss 20492.400391,\t C index 0.5183\n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Training: \tloss 69076.398438,\t C index 0.8478\n",
      "Testing: \tAvg loss 19803.917969,\t C index 0.5200\n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Training: \tloss 61162.828125,\t C index 0.8585\n",
      "Testing: \tAvg loss 19082.011719,\t C index 0.5198\n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Training: \tloss 73946.773438,\t C index 0.8505\n",
      "Testing: \tAvg loss 19536.085938,\t C index 0.5148\n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Training: \tloss 62695.960938,\t C index 0.8536\n",
      "Testing: \tAvg loss 19322.066406,\t C index 0.5198\n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Training: \tloss 64597.328125,\t C index 0.8598\n",
      "Testing: \tAvg loss 19542.490234,\t C index 0.5188\n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Training: \tloss 61275.566406,\t C index 0.8564\n",
      "Testing: \tAvg loss 19979.316406,\t C index 0.5183\n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Training: \tloss 82338.203125,\t C index 0.8308\n",
      "Testing: \tAvg loss 20472.523438,\t C index 0.5138\n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Training: \tloss 66980.476562,\t C index 0.8450\n",
      "Testing: \tAvg loss 20341.839844,\t C index 0.5104\n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Training: \tloss 64249.062500,\t C index 0.8570\n",
      "Testing: \tAvg loss 20464.621094,\t C index 0.5148\n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Training: \tloss 62531.652344,\t C index 0.8564\n",
      "Testing: \tAvg loss 19024.929688,\t C index 0.5232\n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Training: \tloss 71564.515625,\t C index 0.8568\n",
      "Testing: \tAvg loss 19709.966797,\t C index 0.5272\n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Training: \tloss 61735.109375,\t C index 0.8618\n",
      "Testing: \tAvg loss 20478.812500,\t C index 0.5193\n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Training: \tloss 59764.644531,\t C index 0.8500\n",
      "Testing: \tAvg loss 20084.714844,\t C index 0.5178\n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Training: \tloss 61316.843750,\t C index 0.8490\n",
      "Testing: \tAvg loss 20024.505859,\t C index 0.5158\n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Training: \tloss 55613.695312,\t C index 0.8606\n",
      "Testing: \tAvg loss 20498.199219,\t C index 0.5148\n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Training: \tloss 65795.812500,\t C index 0.8472\n",
      "Testing: \tAvg loss 19552.902344,\t C index 0.5178\n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Training: \tloss 59425.398438,\t C index 0.8672\n",
      "Testing: \tAvg loss 18824.300781,\t C index 0.5217\n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Training: \tloss 65492.953125,\t C index 0.8551\n",
      "Testing: \tAvg loss 19424.820312,\t C index 0.5208\n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Training: \tloss 54634.460938,\t C index 0.8627\n",
      "Testing: \tAvg loss 19392.335938,\t C index 0.5222\n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Training: \tloss 60954.210938,\t C index 0.8552\n",
      "Testing: \tAvg loss 19291.650391,\t C index 0.5188\n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Training: \tloss 65052.226562,\t C index 0.8607\n",
      "Testing: \tAvg loss 20826.152344,\t C index 0.5114\n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Training: \tloss 51605.476562,\t C index 0.8491\n",
      "Testing: \tAvg loss 21335.277344,\t C index 0.5099\n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Training: \tloss 53504.132812,\t C index 0.8553\n",
      "Testing: \tAvg loss 21226.357422,\t C index 0.5099\n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Training: \tloss 66181.484375,\t C index 0.8465\n",
      "Testing: \tAvg loss 20924.373047,\t C index 0.5059\n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Training: \tloss 59317.824219,\t C index 0.8588\n",
      "Testing: \tAvg loss 21520.109375,\t C index 0.5094\n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Training: \tloss 65382.574219,\t C index 0.8455\n",
      "Testing: \tAvg loss 20773.917969,\t C index 0.5148\n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Training: \tloss 63524.820312,\t C index 0.8702\n",
      "Testing: \tAvg loss 21339.298828,\t C index 0.5153\n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Training: \tloss 59855.480469,\t C index 0.8613\n",
      "Testing: \tAvg loss 21037.263672,\t C index 0.5193\n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Training: \tloss 63432.500000,\t C index 0.8590\n",
      "Testing: \tAvg loss 21460.871094,\t C index 0.5109\n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Training: \tloss 52394.617188,\t C index 0.8664\n",
      "Testing: \tAvg loss 22508.921875,\t C index 0.4985\n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Training: \tloss 62926.167969,\t C index 0.8531\n",
      "Testing: \tAvg loss 21398.476562,\t C index 0.5000\n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Training: \tloss 62355.421875,\t C index 0.8471\n",
      "Testing: \tAvg loss 21236.583984,\t C index 0.5099\n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Training: \tloss 61293.671875,\t C index 0.8493\n",
      "Testing: \tAvg loss 20548.878906,\t C index 0.5153\n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Training: \tloss 59205.320312,\t C index 0.8735\n",
      "Testing: \tAvg loss 19873.933594,\t C index 0.5161\n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Training: \tloss 51327.285156,\t C index 0.8595\n",
      "Testing: \tAvg loss 20114.453125,\t C index 0.5213\n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Training: \tloss 76402.078125,\t C index 0.8488\n",
      "Testing: \tAvg loss 20154.761719,\t C index 0.5247\n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Training: \tloss 54074.718750,\t C index 0.8575\n",
      "Testing: \tAvg loss 19972.093750,\t C index 0.5245\n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Training: \tloss 56446.648438,\t C index 0.8634\n",
      "Testing: \tAvg loss 20239.796875,\t C index 0.5267\n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Training: \tloss 66942.359375,\t C index 0.8475\n",
      "Testing: \tAvg loss 20286.714844,\t C index 0.5299\n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Training: \tloss 74865.851562,\t C index 0.8441\n",
      "Testing: \tAvg loss 20945.457031,\t C index 0.5247\n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Training: \tloss 66873.789062,\t C index 0.8461\n",
      "Testing: \tAvg loss 20923.089844,\t C index 0.5227\n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Training: \tloss 66274.640625,\t C index 0.8597\n",
      "Testing: \tAvg loss 19970.570312,\t C index 0.5262\n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Training: \tloss 58532.007812,\t C index 0.8449\n",
      "Testing: \tAvg loss 19156.205078,\t C index 0.5321\n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Training: \tloss 66612.062500,\t C index 0.8563\n",
      "Testing: \tAvg loss 19365.472656,\t C index 0.5255\n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Training: \tloss 67314.015625,\t C index 0.8476\n",
      "Testing: \tAvg loss 20825.863281,\t C index 0.5178\n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Training: \tloss 60495.101562,\t C index 0.8625\n",
      "Testing: \tAvg loss 21787.531250,\t C index 0.5183\n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Training: \tloss 52340.539062,\t C index 0.8562\n",
      "Testing: \tAvg loss 20998.498047,\t C index 0.5222\n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Training: \tloss 52131.988281,\t C index 0.8561\n",
      "Testing: \tAvg loss 20843.523438,\t C index 0.5213\n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Training: \tloss 54949.257812,\t C index 0.8580\n",
      "Testing: \tAvg loss 20295.683594,\t C index 0.5217\n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Training: \tloss 45186.925781,\t C index 0.8672\n",
      "Testing: \tAvg loss 20634.425781,\t C index 0.5193\n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Training: \tloss 52609.156250,\t C index 0.8604\n",
      "Testing: \tAvg loss 19789.855469,\t C index 0.5151\n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Training: \tloss 57315.710938,\t C index 0.8597\n",
      "Testing: \tAvg loss 19810.074219,\t C index 0.5094\n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Training: \tloss 58927.343750,\t C index 0.8545\n",
      "Testing: \tAvg loss 19827.375000,\t C index 0.5094\n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Training: \tloss 54011.726562,\t C index 0.8614\n",
      "Testing: \tAvg loss 19890.580078,\t C index 0.5198\n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Training: \tloss 60227.507812,\t C index 0.8575\n",
      "Testing: \tAvg loss 20733.208984,\t C index 0.5153\n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Training: \tloss 62582.710938,\t C index 0.8601\n",
      "Testing: \tAvg loss 21282.111328,\t C index 0.5237\n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Training: \tloss 45136.449219,\t C index 0.8727\n",
      "Testing: \tAvg loss 21357.382812,\t C index 0.5203\n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Training: \tloss 57028.937500,\t C index 0.8535\n",
      "Testing: \tAvg loss 21067.941406,\t C index 0.5153\n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Training: \tloss 56680.773438,\t C index 0.8540\n",
      "Testing: \tAvg loss 21110.320312,\t C index 0.5153\n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Training: \tloss 50458.847656,\t C index 0.8630\n",
      "Testing: \tAvg loss 23151.439453,\t C index 0.5124\n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Training: \tloss 59939.929688,\t C index 0.8642\n",
      "Testing: \tAvg loss 22985.511719,\t C index 0.5062\n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Training: \tloss 68927.593750,\t C index 0.8418\n",
      "Testing: \tAvg loss 22631.539062,\t C index 0.5082\n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Training: \tloss 47295.921875,\t C index 0.8669\n",
      "Testing: \tAvg loss 22355.765625,\t C index 0.5166\n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Training: \tloss 57995.062500,\t C index 0.8600\n",
      "Testing: \tAvg loss 22808.001953,\t C index 0.5020\n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Training: \tloss 51453.292969,\t C index 0.8470\n",
      "Testing: \tAvg loss 22904.976562,\t C index 0.4973\n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Training: \tloss 46511.703125,\t C index 0.8627\n",
      "Testing: \tAvg loss 23045.117188,\t C index 0.5012\n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Training: \tloss 61353.640625,\t C index 0.8551\n",
      "Testing: \tAvg loss 23486.162109,\t C index 0.5032\n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Training: \tloss 63264.984375,\t C index 0.8504\n",
      "Testing: \tAvg loss 21427.867188,\t C index 0.5064\n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Training: \tloss 42841.093750,\t C index 0.8661\n",
      "Testing: \tAvg loss 22510.671875,\t C index 0.5015\n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Training: \tloss 53504.046875,\t C index 0.8621\n",
      "Testing: \tAvg loss 21931.250000,\t C index 0.5087\n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Training: \tloss 36171.425781,\t C index 0.8730\n",
      "Testing: \tAvg loss 22059.199219,\t C index 0.5087\n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Training: \tloss 63114.421875,\t C index 0.8619\n",
      "Testing: \tAvg loss 22948.529297,\t C index 0.5020\n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Training: \tloss 47303.234375,\t C index 0.8654\n",
      "Testing: \tAvg loss 22217.011719,\t C index 0.5124\n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Training: \tloss 50990.453125,\t C index 0.8660\n",
      "Testing: \tAvg loss 24038.486328,\t C index 0.4985\n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Training: \tloss 57690.789062,\t C index 0.8444\n",
      "Testing: \tAvg loss 23936.914062,\t C index 0.5111\n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Training: \tloss 60034.148438,\t C index 0.8624\n",
      "Testing: \tAvg loss 22207.826172,\t C index 0.5153\n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Training: \tloss 52282.492188,\t C index 0.8643\n",
      "Testing: \tAvg loss 21455.177734,\t C index 0.5114\n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Training: \tloss 58682.089844,\t C index 0.8393\n",
      "Testing: \tAvg loss 21258.332031,\t C index 0.5193\n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Training: \tloss 58875.074219,\t C index 0.8603\n",
      "Testing: \tAvg loss 21733.757812,\t C index 0.5277\n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Training: \tloss 49542.859375,\t C index 0.8580\n",
      "Testing: \tAvg loss 22279.902344,\t C index 0.5217\n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Training: \tloss 54485.953125,\t C index 0.8506\n",
      "Testing: \tAvg loss 23809.921875,\t C index 0.5109\n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Training: \tloss 44653.812500,\t C index 0.8623\n",
      "Testing: \tAvg loss 22947.414062,\t C index 0.5044\n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Training: \tloss 50493.718750,\t C index 0.8537\n",
      "Testing: \tAvg loss 21871.386719,\t C index 0.5087\n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Training: \tloss 53528.097656,\t C index 0.8579\n",
      "Testing: \tAvg loss 21434.296875,\t C index 0.5148\n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Training: \tloss 58037.773438,\t C index 0.8522\n",
      "Testing: \tAvg loss 21478.871094,\t C index 0.5163\n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Training: \tloss 60468.027344,\t C index 0.8518\n",
      "Testing: \tAvg loss 21043.939453,\t C index 0.5235\n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Training: \tloss 40706.625000,\t C index 0.8604\n",
      "Testing: \tAvg loss 21929.136719,\t C index 0.5171\n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Training: \tloss 54761.144531,\t C index 0.8515\n",
      "Testing: \tAvg loss 21706.410156,\t C index 0.5131\n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Training: \tloss 54641.042969,\t C index 0.8490\n",
      "Testing: \tAvg loss 22470.687500,\t C index 0.5185\n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Training: \tloss 53507.394531,\t C index 0.8538\n",
      "Testing: \tAvg loss 21993.078125,\t C index 0.5269\n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Training: \tloss 53619.789062,\t C index 0.8622\n",
      "Testing: \tAvg loss 22381.578125,\t C index 0.5235\n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Training: \tloss 49703.882812,\t C index 0.8495\n",
      "Testing: \tAvg loss 21310.804688,\t C index 0.5230\n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Training: \tloss 58578.953125,\t C index 0.8434\n",
      "Testing: \tAvg loss 21119.998047,\t C index 0.5203\n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Training: \tloss 50991.500000,\t C index 0.8596\n",
      "Testing: \tAvg loss 21130.757812,\t C index 0.5156\n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Training: \tloss 47150.148438,\t C index 0.8775\n",
      "Testing: \tAvg loss 21882.466797,\t C index 0.5227\n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Training: \tloss 59212.320312,\t C index 0.8614\n",
      "Testing: \tAvg loss 21360.957031,\t C index 0.5267\n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Training: \tloss 54359.375000,\t C index 0.8544\n",
      "Testing: \tAvg loss 20929.859375,\t C index 0.5217\n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Training: \tloss 52552.578125,\t C index 0.8581\n",
      "Testing: \tAvg loss 21680.597656,\t C index 0.5252\n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Training: \tloss 55327.882812,\t C index 0.8645\n",
      "Testing: \tAvg loss 22350.773438,\t C index 0.5232\n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Training: \tloss 75774.437500,\t C index 0.8514\n",
      "Testing: \tAvg loss 22729.210938,\t C index 0.5148\n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Training: \tloss 57150.273438,\t C index 0.8540\n",
      "Testing: \tAvg loss 22007.886719,\t C index 0.5193\n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Training: \tloss 47312.914062,\t C index 0.8660\n",
      "Testing: \tAvg loss 22675.265625,\t C index 0.5166\n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Training: \tloss 74237.046875,\t C index 0.8565\n",
      "Testing: \tAvg loss 22268.085938,\t C index 0.5282\n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Training: \tloss 56782.410156,\t C index 0.8652\n",
      "Testing: \tAvg loss 21085.332031,\t C index 0.5366\n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Training: \tloss 59851.078125,\t C index 0.8448\n",
      "Testing: \tAvg loss 21588.566406,\t C index 0.5193\n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Training: \tloss 64379.859375,\t C index 0.8597\n",
      "Testing: \tAvg loss 20990.359375,\t C index 0.5230\n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Training: \tloss 65621.593750,\t C index 0.8447\n",
      "Testing: \tAvg loss 21406.482422,\t C index 0.5183\n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Training: \tloss 48611.855469,\t C index 0.8618\n",
      "Testing: \tAvg loss 21829.173828,\t C index 0.5247\n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Training: \tloss 48775.992188,\t C index 0.8510\n",
      "Testing: \tAvg loss 21720.220703,\t C index 0.5213\n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Training: \tloss 67822.203125,\t C index 0.8510\n",
      "Testing: \tAvg loss 21449.546875,\t C index 0.5227\n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Training: \tloss 53004.238281,\t C index 0.8411\n",
      "Testing: \tAvg loss 20304.462891,\t C index 0.5373\n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Training: \tloss 58069.164062,\t C index 0.8555\n",
      "Testing: \tAvg loss 21437.710938,\t C index 0.5267\n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Training: \tloss 46526.808594,\t C index 0.8638\n",
      "Testing: \tAvg loss 21853.296875,\t C index 0.5193\n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Training: \tloss 68231.523438,\t C index 0.8495\n",
      "Testing: \tAvg loss 21099.023438,\t C index 0.5274\n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Training: \tloss 49865.578125,\t C index 0.8609\n",
      "Testing: \tAvg loss 21193.597656,\t C index 0.5299\n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Training: \tloss 49873.203125,\t C index 0.8653\n",
      "Testing: \tAvg loss 22046.302734,\t C index 0.5205\n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Training: \tloss 71047.812500,\t C index 0.8486\n",
      "Testing: \tAvg loss 20240.796875,\t C index 0.5217\n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Training: \tloss 38159.378906,\t C index 0.8722\n",
      "Testing: \tAvg loss 20680.343750,\t C index 0.5262\n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Training: \tloss 50344.617188,\t C index 0.8625\n",
      "Testing: \tAvg loss 21687.062500,\t C index 0.5180\n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Training: \tloss 59176.640625,\t C index 0.8599\n",
      "Testing: \tAvg loss 20912.683594,\t C index 0.5084\n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Training: \tloss 51127.324219,\t C index 0.8574\n",
      "Testing: \tAvg loss 21758.953125,\t C index 0.5099\n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Training: \tloss 39635.964844,\t C index 0.8711\n",
      "Testing: \tAvg loss 21723.746094,\t C index 0.5163\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#training neural network\n",
    "best_C_index=0.5\n",
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader_train_surv, model, loss_train, optimizer)\n",
    "    temp_c_index=test(dataloader_test_surv, model, loss_test)\n",
    "    if temp_c_index>best_C_index:\n",
    "        torch.save(model.state_dict(),\"surv_sub%.4f.pt\"%(temp_c_index))\n",
    "        best_C_index=temp_c_index\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
